# Настройка локальной Llama для чата

## Быстрый старт

### 1. Запустите Ollama

```bash
# На macOS (обычно уже запущен автоматически)
open -a Ollama

# Или через терминал
ollama serve
```

### 2. Установите модель Llama

```bash
# Рекомендуемая модель (быстрая и качественная)
ollama pull llama3.2

# Или другие варианты:
# ollama pull llama3.1    # 8B параметров
# ollama pull llama2      # 7B параметров  
# ollama pull qwen2.5     # Хорошая поддержка русского
```

### 3. Проверьте работу

```bash
# Проверьте что Ollama работает
curl http://localhost:11434/api/tags

# Проверьте установленные модели
ollama list

# Протестируйте модель
ollama run llama3.2 "Привет, как дела?"
```

### 4. Настройка (опционально)

Если нужно изменить модель или URL, создайте файл `backend/.env`:

```env
OLLAMA_API_URL=http://localhost:11434/api/generate
LLM_MODEL=llama3.2
```

По умолчанию используется:
- URL: `http://localhost:11434/api/generate`
- Модель: `llama3.2`

## Готово! 

После этого чат в мобильном приложении будет использовать локальную Llama модель.

**Примечание:** Если Ollama не запущен, система автоматически будет использовать умные fallback-ответы на основе данных из базы данных.
